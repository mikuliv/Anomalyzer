import os
os.environ["OPENBLAS_NUM_THREADS"] = "16"

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import sys

print("‚úÖ –ò–º–ø–æ—Ä—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã")

class BetterMLP(nn.Module):
    def __init__(self, input_size, num_classes):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.4),

            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        return self.net(x)

def print_progress(epoch, batch_idx, total_batches, loss):
    percent = int((batch_idx / total_batches) * 100)
    bar = "‚ñà" * (percent // 4) + "-" * (25 - (percent // 4))
    print(f"\rEpoch {epoch+1}: |{bar}| {percent}% - Batch {batch_idx}/{total_batches} - Loss: {loss:.2f}", end="")

if __name__ == "__main__":
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    df = pd.read_csv("C:/your/path/CICIDS2017_FULL_prepared.csv")
    X = df.drop(columns=["Label"])
    y = df["Label"]

    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)
    class_names = [str(c) for c in label_encoder.classes_]
    np.save("classes.npy", class_names)
    print("‚úÖ –ú–µ—Ç–∫–∏ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã")

    print("üßº –û—á–∏—Å—Ç–∫–∞ NaN –∏ Inf...")
    X.replace([np.inf, -np.inf], np.nan, inplace=True)
    X = X.fillna(X.median())
    print("‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    print("üìä –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    joblib.dump(scaler, "scaler.pkl")
    print("‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

    print("‚öñ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ SMOTE...")
    smote = SMOTE()
    X_bal, y_bal = smote.fit_resample(X_scaled, y)
    print("‚úÖ SMOTE –∑–∞–≤–µ—Ä—à—ë–Ω")

    print("üß™ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ train/test...")
    X_train, X_test, y_train, y_test = train_test_split(
        X_bal, y_bal, test_size=0.2, stratify=y_bal, random_state=42
    )
    print("‚úÖ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

    print("üì¶ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–Ω–∑–æ—Ä–æ–≤...")
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.long)
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test, dtype=torch.long)

    train_ds = TensorDataset(X_train_tensor, y_train_tensor)
    test_ds = TensorDataset(X_test_tensor, y_test_tensor)
    train_dl = DataLoader(train_ds, batch_size=512, shuffle=True, num_workers=0)
    test_dl = DataLoader(test_ds, batch_size=512, num_workers=0)
    print("‚úÖ –¢–µ–Ω–∑–æ—Ä—ã –≥–æ—Ç–æ–≤—ã")

    print("üß† –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = BetterMLP(X_train.shape[1], len(class_names))
    loss_fn = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞")

    print("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è")
    epochs = 30
    losses = []
    try:
        for epoch in range(epochs):
            model.train()
            total_loss = 0
            total_batches = len(train_dl)
            for i, (xb, yb) in enumerate(train_dl):
                preds = model(xb)
                loss = loss_fn(preds, yb)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
                print_progress(epoch, i + 1, total_batches, total_loss)
            losses.append(total_loss)
            print(f"  ‚úÖ Epoch {epoch+1} complete. Total loss: {total_loss:.2f}")
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
        sys.exit(1)

    print("üîç –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏")
    model.eval()
    y_pred = []
    with torch.no_grad():
        for xb, _ in test_dl:
            preds = model(xb)
            y_pred.extend(torch.argmax(preds, dim=1).cpu().numpy())

    y_test_np = y_test_tensor.cpu().numpy()

    print("\n=== Classification Report ===")
    print(classification_report(y_test_np, y_pred, target_names=class_names))

    print("üìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫")
    cm = confusion_matrix(y_test_np, y_pred)
    plt.figure(figsize=(14, 10))
    sns.heatmap(cm, annot=False, cmap="Blues", xticklabels=class_names, yticklabels=class_names)
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted Class")
    plt.ylabel("True Class")
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig("confusion_matrix.png")
    print("‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ confusion_matrix.png")

    plt.figure()
    plt.plot(range(1, epochs+1), losses, marker='o')
    plt.title("Training Loss per Epoch")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("loss_plot.png")
    print("‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–∫ loss_plot.png")

    torch.save(model.state_dict(), "mlp_cicids.pt")
    torch.save(model, "mlp_cicids_full.pt")
    print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ state_dict –∏ –∫–∞–∫ –ø–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å.")
